---
title: "STAT 598: Assignment 3"
author: Walter Griebenow (UIN 656730939 MCS-DS), Dillon Harding (UIN 659588546 MCS-DS), Adi Johnson (UIN 651099519 MCS-DS)
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r }
library(splines)
library(dplyr)
library(tidyverse)
library(ggplot2)

options(digits = 4)
set.seed(939)
```

# Part I

```{r }
myData = read.csv("Coding3_Data.csv")
x = myData$x
y = myData$y

fx = 1:100/100;
fy = sin(12*(fx+0.2))/(fx+0.2)

```


### Local regression with LOESS.

```{r }
smooth.diagonal = function(x, span){
# return the diagonal of the smooth matrix with values x and span = span
# 
 n = length(x)
 A = matrix(0, n, n)
 for(i in 1:n){
       y = rep(0, n); y[i]=1
       yi = loess(y ~ x, data.frame(x, y), span=span)$fitted
       A[,i]= yi
       }
 return(diag(A))
}

```


```{r }
spans = seq(0.2, 0.9, by = 0.05)

m = length(spans)
n = length(x)

mycv = rep(0,m)
mygcv = rep(0,m)

for (i in 1:m) {
  S = smooth.diagonal(x, span = spans[i])
  mean_S = mean(S)
  fit = loess(y ~ x, myData, span=spans[i])
  mycv[i] = sum(((y-fit$fitted)/(1-S))^2)/n
  mygcv[i] = sum((y-fit$fitted)^2)/(1-mean_S)^2/n
}
  
optspan.gcv = spans[mygcv==min(mygcv)]
optspan.cv = spans[mycv==min(mycv)]

fitopt = loess(y ~ x, myData, span=optspan.cv)

print('CV error:')
mycv

print ('GCV error:')
mygcv

print('optimal span based on CV:')
optspan.cv

print('optimal span based on GCV:')
optspan.gcv
```



```{r }

plot(x, y, xlab="x", ylab="y")
lines(fx, predict(fitopt, fx), col="blue", lwd=2, lty = 2)
lines(fx, fy, col="gray", lwd=2)

```

# Part II
```{r}
sales = read.csv('Sales_Transactions_Dataset_Weekly.csv')[,c(1:53)] %>% 
  data.frame()

# Normalize time series by removing means -----------
# Store as 811 x 52 matrix
sales_norm = sales %>% 
  mutate(means = rowMeans(select(., starts_with('W')), na.rm=T)) %>% 
  mutate(across(c(-c('Product_Code', 'means')), ~ . - means)) %>% 
  select(-means) %>% 
  column_to_rownames('Product_Code') %>% 
  as.matrix()  
```

### 1. Fitting NCS

```{r}
# 1. Fitting NCS 
n.knots = 8
df = 9
x = seq(0, 52, length.out= ncol(sales_norm))

## F is a 52-by-9 design matrix without the intercept. For instance, this can be obtained using
# the ns command in R. Remove the column mean from F to disregard the intercept.
mat_F1 = ns(x, df=9, intercept=FALSE)
mat_F = t(t(mat_F1) - colMeans(mat_F1)) 

# Get B using provided matrix equation and mat_F
eq1 = solve(t(mat_F)%*%mat_F)
B_transpose = eq1%*%t(mat_F)%*%t(sales_norm)
mat_B = t(B_transpose)
```

## Clustering
```{r}
plot_function <- function(df, i, matrix = 'B'){
  matplot(df, type='l', col='grey', pch=20, xlab='Weeks', ylab='Weekly Sales',
          ylim = c(-20,30))
  if (matrix == 'B'){
    # use matrix product Fb
    matlines(mat_F%*%Bcenters[i,], type='l', col='red', lwd=1, lty=1) 
  }
  else{
    matlines(rowMeans(df), type='l', col='red', lwd=1, lty=1) 
  }
}
```

### 2. Clustering Using Matrix B 

```{r}
# K-means on matrix B with 6 clusters
kmeans_B = kmeans(mat_B, centers = 6, nstart=5)
Bclusters = kmeans_B$cluster
Bcenters = kmeans_B$centers
```

```{r}
par(mfrow=c(2,3))
for (i in 1:6){
  pick_cluster= which(Bclusters==i)
  plot_data = matrix(0,52,length(pick_cluster))

  for (j in 1:length(pick_cluster)){
    plot_data[,j] = sales_norm[j,]
  }

  plot_data_final = as.data.frame(plot_data) 
  names(plot_data_final) = gsub(pattern="V", replacement="", x=names(plot_data_final))

  plot_function(plot_data_final, i, matrix='B')
}
```

### 3. Clustering Using Matrix X
```{r}
kmeans_X = kmeans(sales_norm, centers = 6, nstart=5)
Xclusters = kmeans_X$cluster
Xcenters = kmeans_X$centers
```

```{r}
par(mfrow=c(2,3))
for (i in 1:6){
  pick_cluster= which(Xclusters==i)
  plot_data = matrix(0,52,length(pick_cluster))

  for (j in 1:length(pick_cluster)){
    plot_data[,j] = sales_norm[j,]
  }

  plot_data_final = as.data.frame(plot_data)

  plot_function(plot_data_final, i, matrix='X')
}
```

# Part III
### 1. Ridgless Function
```{r}

ridgeless <- function(train_data, test_data) {
  
  # Separate response variable Y from predictors X
  X_train <- train_data[, -1]
  Y_train <- train_data[, 1]
  X_test <- test_data[, -1]
  Y_test <- test_data[, 1]
  
  # Center each column of the design matrix from the training data without scaling
  X_train_scaled <- scale(X_train, center = TRUE, scale = FALSE)
  X_test_scaled <- scale(X_test, center = TRUE, scale = FALSE)
  
  # Perform PCA on the centered training data
  pca_result <- prcomp(X_train_scaled, center = TRUE, scale = FALSE)
  
  # Calculate the number of components to keep using eigenvalues
  eigenvalues <- pca_result$sdev^2
  eps <- 1e-10  # Threshold for singular values
  num_components <- sum(eigenvalues > eps)
  
  # Use only the top num_components for both train and test data
  pca_train <- pca_result$x[, 1:num_components]
  pca_test <- predict(pca_result, newdata = X_test_scaled)[, 1:num_components]
  
  # Perform PCR 
  lam <- diag(1/diag(t(pca_train) %*% pca_train))
  beta <- lam %*% t(pca_train) %*% Y_train
  
  # Calculate training and test errors
  y_hat_train <- pca_train %*% beta
  test_predictions <- pca_test %*% beta
  
  #Mean Squared Error for test and train
  train_error <- mean((y_hat_train - Y_train)^2)
  test_error <- mean((test_predictions - Y_test)^2)
  
  return(list("Training Error" = train_error, "Test Error" = test_error))
}
```

### 2. Simluation Study
```{r}
myData = read.csv('Coding3_dataH.csv')
#Number of iterations for Simulation Study
T = 30

#d ranges from 6 to 241
num_features = 236

#Test Error Matrix
test_errors <- matrix(0, nrow = T, ncol = num_features)

# Calculate the number of rows for the test set (75%)
test_size <- round(0.75 * nrow(myData))


## Perform Simulation Study ##
for (i in 1:T) {
  
  ## Randomly Partition the data into training (25%) and test (75%) ##
  
  # Randomly select row indices for the test set
  test_indices <- sample(1:nrow(myData), test_size)
  
  # Create the test and train sets based on the selected row indices
  test_data <- myData[test_indices, ]
  train_data <- myData[-test_indices, ]
  
  ## Calculate and log the test error from the ridgeless method using the first d columns of myData
  ## d ranges from 6 to 241, since the first column represents Y ##
  
  for (d in 6:241) {
    
    # Create the train and test data with the first d columns
    train_data_sub <- train_data[, 1:d]
    test_data_sub <- test_data[, 1:d]
    
    # Call the ridgeless function
    errors <- ridgeless(train_data_sub, test_data_sub)
    
    # Log the test error for this iteration of i and d
    test_errors[i, d - 5] <- errors$`Test Error`
  }
}
```

```{r}
# Plot the median of the test errors (collated over the 30 iterations) in log scale against the count 
## of the regression parameters, which spans from 5 to 240


# Calculate the median of test errors over the 30 iterations for each value of d
median_test_errors <- apply(log10(test_errors), 2, median)

# Create a data frame for the plot
plot_data <- data.frame(Count_of_Parameters = 6:241, Median_Test_Error = median_test_errors)

# Create the plot of points
ggplot(plot_data, aes(x = Count_of_Parameters, y = Median_Test_Error)) +
  geom_point() +  # Use geom_point() to plot individual points
  scale_x_continuous(breaks = seq(0, 240, by = 20)) +  # Adjust the x-axis breaks as needed
  scale_y_log10() +  # Use a log scale for the y-axis
  labs(x = "Count of Regression Parameters", y = "Median Test Error (log scale)") +
  ggtitle("Median Test Error vs. Count of Regression Parameters") +
  theme_minimal()

# Continuous plot
ggplot(plot_data, aes(x = Count_of_Parameters, y = Median_Test_Error)) +
  geom_line() +
  scale_x_continuous(breaks = seq(0, 240, by = 20)) +  # Adjust the x-axis breaks as needed
  scale_y_log10() +  # Use a log scale for the y-axis
  labs(x = "Count of Regression Parameters", y = "Median Test Error (log scale)") +
  ggtitle("Median Test Error vs. Count of Regression Parameters") +
  theme_minimal()
```
